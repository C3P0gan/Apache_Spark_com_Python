{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b17d55-70cf-4221-8f9b-2a2522f1725e",
   "metadata": {},
   "source": [
    "# Spark NLP: aprenda a otimizar a linguagem natural com eficácia\n",
    "\n",
    "[Spark NLP: aprenda a otimizar a linguagem natural com eficácia](https://www.alura.com.br/artigos/spark-nlp-linguagem-natural-forma-otimizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942b48c-cbb9-48d8-b8ba-0d15029f1883",
   "metadata": {},
   "source": [
    "## Preparando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2932a4bf-c503-4336-a280-c0dc0f2167bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ecc8537-5c00-4d04-889e-f3a3704b7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"The Beatles\", \"There are places I'll remember \" +\n",
    "                        \"All my life though some have changed \" +\n",
    "                        \"Some forever, not for better \" +\n",
    "                        \"Some have gone and some remain\"),\n",
    "       (\"Oasis\", \"So I start a revolution from my bed \" +\n",
    "                 \"Cause you said the brains I had went to my head \" +\n",
    "                 \"Step outside, summertime's in bloom \" +\n",
    "                 \"Stand up beside the fireplace\"),\n",
    "       (\"Pink Floyd\", \"How I wish you were here \" +\n",
    "                      \"We're just two lost soul \" +\n",
    "                      \"Swimming in a fish bowl year after year \" +\n",
    "                      \"Running over the same old gound\")]\n",
    "df_musica = spark.createDataFrame(data, [\"artista\", \"letra\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26003719-e64c-40f8-88f4-87423ba5f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import LemmatizerModel, Stemmer, Tokenizer, StopWordsCleaner\n",
    "from sparknlp.base import DocumentAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a94f3-9d86-4fa0-9745-bb1cd570a8cd",
   "metadata": {},
   "source": [
    "## DocumentAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee810b99-d5f5-4f3d-928f-67b7412f4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[There are places I'll remember All my life though some have changed Some forever, not for better Some have gone and some remain]                      |\n",
      "|[So I start a revolution from my bed Cause you said the brains I had went to my head Step outside, summertime's in bloom Stand up beside the fireplace]|\n",
      "|[How I wish you were here We're just two lost soul Swimming in a fish bowl year after year Running over the same old gound]                            |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"letra\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "doc_df = document_assembler.transform(df_musica)\n",
    "doc_df.select(\"document.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fbbb0-5dc0-4ac5-9afc-0bfb2ad15bc7",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87af4325-e528-4d41-900d-20376450c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[There, are, places, I'll, remember, All, my, life, though, some, have, changed, Some, forever, ,, not, for, better, Some, have, gone, and, some, remain]                            |\n",
      "|[So, I, start, a, revolution, from, my, bed, Cause, you, said, the, brains, I, had, went, to, my, head, Step, outside, ,, summertime's, in, bloom, Stand, up, beside, the, fireplace]|\n",
      "|[How, I, wish, you, were, here, We're, just, two, lost, soul, Swimming, in, a, fish, bowl, year, after, year, Running, over, the, same, old, gound]                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
    "token_df.select(\"token.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98cc94-8546-4965-adb0-32f7f6034bec",
   "metadata": {},
   "source": [
    "## Removendo stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9a1c66-3fd4-4ca2-8460-395ab978a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|[places, remember, life, though, changed, forever, ,, better, gone, remain]                                               |\n",
      "|[start, revolution, bed, Cause, said, brains, went, head, Step, outside, ,, summertime's, bloom, Stand, beside, fireplace]|\n",
      "|[wish, two, lost, soul, Swimming, fish, bowl, year, year, Running, old, gound]                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(\"token\") \\\n",
    "    .setOutputCol(\"clean_tokens\")\n",
    "token_df_clean = stopwords_cleaner.transform(token_df)\n",
    "token_df_clean.select(\"clean_tokens.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da97662-2052-46d3-998d-728c40bf6df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_cleaner.getStopWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f8deb-aa7c-4643-b7dd-1806ae8d2f90",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b71d74-9691-4109-b7b9-1514e809c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|[place, rememb, life, though, chang, forev, ,, better, gone, remain]                                             |\n",
      "|[start, revolut, bed, caus, said, brain, went, head, step, outsid, ,, summertime', bloom, stand, besid, fireplac]|\n",
      "|[wish, two, lost, soul, swim, fish, bowl, year, year, run, old, gound]                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"clean_tokens\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "stem = stemmer.transform(token_df_clean)\n",
    "stem.select(\"stem.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba149a-219f-44ca-aa05-1e537c3177f1",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2501b1e0-b9a7-4063-af06-0c618df93c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ — ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ \\ ]Download done! Loading the resource.\n",
      "[OK!]\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|[place, remember, life, though, change, forever, ,, well, go, remain]                                                 |\n",
      "|[start, revolution, bed, Cause, say, brain, go, head, Step, outside, ,, summertime's, bloom, Stand, beside, fireplace]|\n",
      "|[wish, two, lose, soul, Swimming, fish, bowl, year, year, Running, old, gound]                                        |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"clean_tokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "result = lemmatizer.transform(stem)\n",
    "result.select(\"lemma.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f40bec7-586c-400a-b653-e954efc81eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark: NLP (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
